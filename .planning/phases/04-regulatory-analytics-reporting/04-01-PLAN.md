---
phase: 04-regulatory-analytics-reporting
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - sql/003_regulatory_analytics.sql
  - sql/verify_003_schema.sql
  - sql/apply_and_verify_003.py
autonomous: true

must_haves:
  truths:
    - "Audit trail entries are immutable once created"
    - "Regulatory reference data supports time-series queries"
    - "Model governance tracks version changes with backtesting results"
    - "Alert configuration supports threshold-based monitoring"
  artifacts:
    - path: "sql/003_regulatory_analytics.sql"
      provides: "6 new tables for regulatory analytics"
      min_lines: 200
      contains: "audit_trail, regulatory_reference, model_governance, alert_config, alert_log, regulatory_metrics"
    - path: "sql/verify_003_schema.sql"
      provides: "Schema verification queries"
      min_lines: 80
    - path: "sql/apply_and_verify_003.py"
      provides: "Automated schema application and verification"
      min_lines: 150
  key_links:
    - from: "audit_trail table"
      to: "immutability enforcement"
      via: "PostgreSQL trigger"
      pattern: "prevent_audit_modification"
    - from: "regulatory_reference table"
      to: "time-series queries"
      via: "effective_date index"
      pattern: "CREATE INDEX.*effective_date DESC"
    - from: "model_governance table"
      to: "audit_trail entries"
      via: "model_version foreign key"
      pattern: "REFERENCES model_governance"
---

<objective>
Extend PostgreSQL schema with regulatory analytics tables: immutable audit trail, regulatory reference data (risk weights, PD curves, LGD tables), model governance tracking, and alerting configuration.

Purpose: Provide database foundation for REG-04 (audit trail), REG-05 (model governance), and RPT-04 (alerting). Enable Phase 4 regulatory calculations with full audit compliance.

Output: 6 new tables with indexes, constraints, and verification tools ready for Phase 4 services.
</objective>

<execution_context>
@C:/Users/omack/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/omack/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-regulatory-analytics-reporting/04-RESEARCH.md

# Phase 3 schema foundation
@.planning/phases/03-portfolio-data-services/03-01-SUMMARY.md
@sql/002_portfolio_data_services.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create regulatory analytics schema (003_regulatory_analytics.sql)</name>
  <files>sql/003_regulatory_analytics.sql</files>
  <action>
Create `sql/003_regulatory_analytics.sql` with 6 tables following Phase 3 patterns (idempotent, transaction-wrapped, indexed for performance):

**1. audit_trail** - Immutable append-only log for all regulatory calculations
- audit_id (text PK) - UUID for each audit entry
- audit_type (text CHECK: GAAP, IFRS, CECL, BASEL, MODEL_CHANGE)
- calculation_run_id (text NOT NULL) - Links to run or batch job
- entity_type (text CHECK: POSITION, PORTFOLIO, COUNTERPARTY)
- entity_id (text NOT NULL)
- calculation_method (text NOT NULL) - e.g., "ASC326_MULTI_SCENARIO", "BASEL3_STANDARD"
- input_snapshot_id (text NOT NULL) - Market snapshot used (immutable reference)
- input_version (text NOT NULL) - Data version (model version, curve date)
- assumptions_json (jsonb NOT NULL) - Input assumptions (PD table version, LGD, macro scenario weights)
- results_json (jsonb NOT NULL) - Full calculation output (allowance, RWA, fair value)
- metadata_json (jsonb) - User context (who, why, system version)
- computed_at (timestamptz NOT NULL)
- created_at (timestamptz NOT NULL DEFAULT now())
- Trigger: prevent_audit_modification() blocks UPDATE/DELETE (immutability enforcement per research)
- Index on (calculation_run_id), (entity_type, entity_id), (audit_type, computed_at DESC)

**2. regulatory_reference** - Risk weights, PD curves, LGD tables with versioning
- ref_id (text PK)
- ref_type (text CHECK: RISK_WEIGHT, PD_CURVE, LGD_TABLE, Q_FACTOR)
- entity_key (text NOT NULL) - e.g., "CORPORATE/AAA", "BBB", "SOVEREIGN/AA"
- ref_value (numeric NOT NULL) - Risk weight (0.0-1.5), PD probability, LGD percentage
- effective_date (timestamptz NOT NULL)
- expired_date (timestamptz) - NULL if current
- source (text NOT NULL) - e.g., "BASEL_III_2023", "MOODYS_2026_02"
- metadata_json (jsonb) - Additional parameters
- Index on (ref_type, entity_key, effective_date DESC) for temporal lookups

**3. model_governance** - Model versioning, backtesting, calibration tracking
- model_version (text PK) - e.g., "cecl_v2_2026_02_11"
- model_type (text CHECK: CECL, BASEL_RWA, GAAP_VALUATION, IFRS_VALUATION)
- git_hash (text) - Git commit hash for code reproducibility
- deployment_date (timestamptz NOT NULL)
- approval_status (text CHECK: TESTING, APPROVED, DEPRECATED)
- backtesting_results_json (jsonb) - Performance metrics, error rates
- calibration_date (timestamptz)
- calibration_params_json (jsonb)
- notes (text)
- Index on (model_type, deployment_date DESC)

**4. alert_config** - Threshold configuration for monitoring
- alert_id (text PK)
- alert_type (text CHECK: DURATION_THRESHOLD, CONCENTRATION_LIMIT, CREDIT_DETERIORATION, LIQUIDITY_RATIO)
- portfolio_node_id (text) - Portfolio scope (NULL for global)
- threshold_value (numeric NOT NULL)
- threshold_operator (text CHECK: GT, LT, EQ, GTE, LTE)
- metric_name (text NOT NULL) - e.g., "cet1_ratio", "issuer_concentration_pct"
- notification_channels (text[]) - ["email", "slack"]
- enabled (boolean DEFAULT true)
- created_at (timestamptz DEFAULT now())
- Index on (portfolio_node_id, enabled)

**5. alert_log** - Alert trigger history
- log_id (text PK)
- alert_id (text REFERENCES alert_config)
- triggered_at (timestamptz NOT NULL DEFAULT now())
- metric_value (numeric NOT NULL)
- threshold_value (numeric NOT NULL)
- portfolio_node_id (text)
- position_id (text)
- notification_sent (boolean DEFAULT false)
- notification_sent_at (timestamptz)
- resolved (boolean DEFAULT false)
- resolved_at (timestamptz)
- Index on (alert_id, triggered_at DESC), (portfolio_node_id, resolved)

**6. regulatory_metrics** - Cached regulatory calculations for query performance
- metric_id (text PK)
- metric_type (text CHECK: CECL_ALLOWANCE, BASEL_RWA, CAPITAL_RATIO, GAAP_VALUATION)
- portfolio_node_id (text NOT NULL)
- as_of_date (timestamptz NOT NULL)
- metric_value (numeric NOT NULL)
- metric_breakdown_json (jsonb) - Segment-level details
- calculation_run_id (text) - Link to audit_trail
- created_at (timestamptz DEFAULT now())
- Index on (portfolio_node_id, metric_type, as_of_date DESC)
- UNIQUE on (portfolio_node_id, metric_type, as_of_date) for UPSERT idempotency

Use BEGIN/COMMIT wrapper, IF NOT EXISTS guards, CHECK constraints for enum validation (Phase 3 pattern).
  </action>
  <verify>
```bash
# Syntactic verification (pattern from Phase 3 Plan 01)
grep -c "CREATE TABLE" sql/003_regulatory_analytics.sql
# Expected: 6

grep -c "CREATE INDEX" sql/003_regulatory_analytics.sql
# Expected: >=10

grep -c "CREATE TRIGGER" sql/003_regulatory_analytics.sql
# Expected: 1 (audit_trail immutability)

grep -c "CHECK.*IN" sql/003_regulatory_analytics.sql
# Expected: >=6 (enum constraints)
```
  </verify>
  <done>Schema file exists with 6 tables, immutability trigger for audit_trail, temporal indexes on all time-series tables, CHECK constraints for enums, transaction-wrapped, and idempotent</done>
</task>

<task type="auto">
  <name>Task 2: Create schema verification tools</name>
  <files>
    sql/verify_003_schema.sql
    sql/apply_and_verify_003.py
  </files>
  <action>
Create verification tools following Phase 3 Plan 01 pattern:

**sql/verify_003_schema.sql** - Manual verification queries:
```sql
-- Count tables
SELECT count(*) FROM information_schema.tables
WHERE table_schema = 'public' AND table_name IN
  ('audit_trail', 'regulatory_reference', 'model_governance',
   'alert_config', 'alert_log', 'regulatory_metrics');
-- Expected: 6

-- Count indexes
SELECT count(*) FROM pg_indexes WHERE schemaname = 'public'
  AND tablename IN ('audit_trail', 'regulatory_reference', ...);
-- Expected: >=10

-- List triggers
SELECT trigger_name, event_manipulation, event_object_table
FROM information_schema.triggers
WHERE trigger_name LIKE '%audit%';
-- Expected: prevent_audit_modification trigger on audit_trail

-- Test immutability (should fail)
BEGIN;
INSERT INTO audit_trail (audit_id, audit_type, calculation_run_id, entity_type, entity_id,
  calculation_method, input_snapshot_id, input_version, assumptions_json, results_json, computed_at)
VALUES ('test-audit-1', 'CECL', 'run-1', 'PORTFOLIO', 'port-1', 'ASC326', 'snap-1', 'v1',
  '{}', '{}', now());
-- Try update (should raise exception)
UPDATE audit_trail SET audit_type = 'BASEL' WHERE audit_id = 'test-audit-1';
ROLLBACK;

-- Test CHECK constraints
BEGIN;
INSERT INTO audit_trail (..., audit_type = 'INVALID_TYPE', ...);
-- Should fail
ROLLBACK;
```

**sql/apply_and_verify_003.py** - Automated verification:
```python
import os
import psycopg
from psycopg import sql
from psycopg.rows import dict_row

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/iprs")

def apply_migration():
    """Apply 003 schema migration."""
    with open("sql/003_regulatory_analytics.sql", "r") as f:
        migration_sql = f.read()

    with psycopg.connect(DATABASE_URL) as conn:
        conn.execute(migration_sql)
        conn.commit()
    print("✓ Migration applied")

def verify_tables():
    """Verify 6 tables exist."""
    expected_tables = {
        'audit_trail', 'regulatory_reference', 'model_governance',
        'alert_config', 'alert_log', 'regulatory_metrics'
    }

    with psycopg.connect(DATABASE_URL, row_factory=dict_row) as conn:
        result = conn.execute("""
            SELECT table_name FROM information_schema.tables
            WHERE table_schema = 'public' AND table_name = ANY(%(tables)s)
        """, {'tables': list(expected_tables)}).fetchall()

        found_tables = {row['table_name'] for row in result}
        missing = expected_tables - found_tables

        if missing:
            print(f"✗ Missing tables: {missing}")
            return False
        print(f"✓ All 6 tables exist: {found_tables}")
        return True

def verify_indexes():
    """Verify indexes exist."""
    with psycopg.connect(DATABASE_URL, row_factory=dict_row) as conn:
        result = conn.execute("""
            SELECT count(*) as idx_count FROM pg_indexes
            WHERE schemaname = 'public' AND tablename IN
            ('audit_trail', 'regulatory_reference', 'model_governance',
             'alert_config', 'alert_log', 'regulatory_metrics')
        """).fetchone()

        count = result['idx_count']
        if count < 10:
            print(f"✗ Expected >=10 indexes, found {count}")
            return False
        print(f"✓ Found {count} indexes")
        return True

def verify_immutability_trigger():
    """Test audit_trail immutability trigger."""
    with psycopg.connect(DATABASE_URL) as conn:
        try:
            conn.execute("BEGIN")
            conn.execute("""
                INSERT INTO audit_trail (audit_id, audit_type, calculation_run_id,
                  entity_type, entity_id, calculation_method, input_snapshot_id,
                  input_version, assumptions_json, results_json, computed_at)
                VALUES ('test-immutability', 'CECL', 'run-test', 'PORTFOLIO', 'port-test',
                  'ASC326', 'snap-test', 'v1', '{}', '{}', now())
            """)

            # Try to update (should fail)
            conn.execute("""
                UPDATE audit_trail SET audit_type = 'BASEL'
                WHERE audit_id = 'test-immutability'
            """)
            conn.execute("ROLLBACK")
            print("✗ Immutability trigger not working (UPDATE succeeded)")
            return False
        except Exception as e:
            if "immutable" in str(e).lower():
                print("✓ Immutability trigger working (UPDATE blocked)")
                conn.execute("ROLLBACK")
                return True
            print(f"✗ Unexpected error: {e}")
            conn.execute("ROLLBACK")
            return False

if __name__ == "__main__":
    print("Applying Phase 4 schema migration...")
    apply_migration()

    print("\nVerifying schema...")
    checks = [
        verify_tables(),
        verify_indexes(),
        verify_immutability_trigger(),
    ]

    if all(checks):
        print("\n✓ All verification checks passed")
    else:
        print("\n✗ Some verification checks failed")
        exit(1)
```
  </action>
  <verify>
```bash
# File existence
test -f sql/verify_003_schema.sql && echo "FOUND: verify_003_schema.sql" || echo "MISSING"
test -f sql/apply_and_verify_003.py && echo "FOUND: apply_and_verify_003.py" || echo "MISSING"

# Python syntax check
python -m py_compile sql/apply_and_verify_003.py
```
  </verify>
  <done>Verification tools created (SQL queries and Python script), Python script syntax-valid, ready to run when database accessible</done>
</task>

<task type="auto">
  <name>Task 3: Commit schema extension and verification tools</name>
  <files>
    .planning/phases/04-regulatory-analytics-reporting/04-01-PLAN.md
    sql/003_regulatory_analytics.sql
    sql/verify_003_schema.sql
    sql/apply_and_verify_003.py
  </files>
  <action>
Commit all Phase 4 schema files with message following Phase 3 pattern:

```bash
node C:/Users/omack/.claude/get-shit-done/bin/gsd-tools.js commit \
  "feat(04-01): regulatory analytics schema extension with audit trail and model governance" \
  --files sql/003_regulatory_analytics.sql sql/verify_003_schema.sql sql/apply_and_verify_003.py
```

Schema is ready for application when database is accessible (same pattern as Phase 3 Plan 01).
  </action>
  <verify>
```bash
# Verify commit exists
git log --oneline -1 | grep -q "04-01" && echo "Commit found" || echo "Commit missing"

# Verify files in git
git ls-files | grep -q "003_regulatory_analytics.sql" && echo "Schema tracked" || echo "Missing"
```
  </verify>
  <done>Schema files committed to git with descriptive message, ready for Phase 4 Plan 02 regulatory compute modules</done>
</task>

</tasks>

<verification>
# Schema structure (syntactic)
- [ ] 6 tables defined (audit_trail, regulatory_reference, model_governance, alert_config, alert_log, regulatory_metrics)
- [ ] >=10 indexes created for temporal queries and lookups
- [ ] Immutability trigger on audit_trail (prevent_audit_modification)
- [ ] CHECK constraints for enum validation (>=6 constraints)
- [ ] Transaction-wrapped with BEGIN/COMMIT
- [ ] Idempotent with IF NOT EXISTS guards

# Verification tools
- [ ] verify_003_schema.sql contains manual verification queries
- [ ] apply_and_verify_003.py contains automated verification
- [ ] Python script syntax-valid (py_compile passes)

# Git tracking
- [ ] Schema files committed
- [ ] Verification tools committed
</verification>

<success_criteria>
1. All 6 Phase 4 tables defined with correct structure
2. Audit trail immutability enforced via PostgreSQL trigger
3. Regulatory reference data supports time-series queries (effective_date index)
4. Model governance tracks version changes with backtesting results
5. Alert configuration schema supports threshold-based monitoring
6. Verification tools ready to apply schema when database accessible
</success_criteria>

<output>
After completion, create `.planning/phases/04-regulatory-analytics-reporting/04-01-SUMMARY.md`
</output>
