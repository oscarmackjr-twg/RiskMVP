---
phase: 03-portfolio-data-services
plan: 04
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - services/data_ingestion_svc/app/routes/market_feeds.py
  - services/data_ingestion_svc/app/routes/loan_servicing.py
  - services/data_ingestion_svc/app/routes/lineage.py
  - services/data_ingestion_svc/app/routes/vendor.py
  - services/data_ingestion_svc/app/models.py
  - services/common/hash.py
autonomous: true

must_haves:
  truths:
    - "User can upload yield curves and credit spreads via API"
    - "User can ingest loan servicing data with validation"
    - "User can query data lineage to trace data sources"
    - "User can view ingestion batch status and errors"
    - "All ingested data has audit trail with timestamps and sources"
  artifacts:
    - path: "services/data_ingestion_svc/app/routes/market_feeds.py"
      provides: "Market data feed ingestion endpoints"
      exports: ["ingest_yield_curve", "ingest_credit_spreads", "ingest_rating_change"]
    - path: "services/data_ingestion_svc/app/routes/lineage.py"
      provides: "Data lineage query endpoints"
      exports: ["get_lineage_graph", "trace_position_lineage"]
    - path: "services/common/hash.py"
      provides: "Content-addressable hashing utility"
      exports: ["sha256_json"]
  key_links:
    - from: "services/data_ingestion_svc/app/routes/market_feeds.py"
      to: "market_data_feed table"
      via: "UPSERT with payload_hash deduplication"
      pattern: "ON CONFLICT (feed_id) DO UPDATE"
    - from: "ingestion endpoints"
      to: "data_lineage table"
      via: "INSERT lineage record after successful ingestion"
      pattern: "transformation_chain = ['RECEIVE', 'VALIDATE', 'PARSE', 'STORE']"
---

<objective>
Implement data ingestion service for market feeds (yield curves, credit spreads, ratings) and loan servicing data. Enable data lineage tracking and audit trails for regulatory compliance.

Purpose: Data integration layer for Phase 3; enables historical data versioning and traceability.
Output: Working data ingestion service with validation, lineage tracking, and batch status monitoring.
</objective>

<execution_context>
@C:/Users/omack/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/omack/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-portfolio-data-services/03-RESEARCH.md
@.planning/phases/03-portfolio-data-services/03-01-SUMMARY.md

# Database schema
@sql/002_portfolio_data_services.sql

# Service scaffolding
@services/data_ingestion_svc/app/main.py
@services/data_ingestion_svc/app/routes/market_feeds.py
@services/data_ingestion_svc/app/routes/loan_servicing.py
@services/data_ingestion_svc/app/routes/lineage.py
@services/data_ingestion_svc/app/routes/vendor.py
@services/data_ingestion_svc/app/models.py

# Shared utilities
@services/common/db.py
@services/common/service_base.py
@services/common/hash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement market data feed ingestion with lineage</name>
  <files>
services/data_ingestion_svc/app/routes/market_feeds.py
services/common/hash.py
  </files>
  <action>
Replace stub implementations in market_feeds.py with working endpoints for yield curves, credit spreads, and ratings ingestion.

**First verify/create hash.py utility:**
Check if services/common/hash.py exists. If not, create it with:
```python
import hashlib
import json

def sha256_json(obj: dict) -> str:
    """Compute SHA-256 hash of JSON object with deterministic serialization."""
    canonical = json.dumps(obj, sort_keys=True, separators=(',', ':'))
    return hashlib.sha256(canonical.encode('utf-8')).hexdigest()
```

**market_feeds.py endpoints:**

1. `POST /api/v1/ingestion/market-feeds/yield-curves` - Ingest yield curve
   - Accept YieldCurveUpload (curve_id, curve_type, as_of_date, source, nodes: List[{tenor, rate}])
   - Validate: curve_type IN ('DISCOUNT', 'FORWARD', 'BASIS', 'SPREAD')
   - Validate: as_of_date is valid datetime
   - Validate: nodes has at least 2 points
   - Compute payload_hash = sha256_json(req.model_dump())
   - UPSERT into market_data_feed:
     ```sql
     INSERT INTO market_data_feed
       (feed_id, feed_type, as_of_date, source, payload_json, payload_hash, validation_status, created_at)
     VALUES (%(fid)s, 'YIELD_CURVE', %(aof)s, %(src)s, %(pl)s::jsonb, %(ph)s, 'PASS', now())
     ON CONFLICT (feed_id) DO UPDATE SET
       payload_json = EXCLUDED.payload_json,
       payload_hash = EXCLUDED.payload_hash,
       validation_status = EXCLUDED.validation_status,
       updated_at = now();
     ```
   - Record lineage:
     ```sql
     INSERT INTO data_lineage
       (lineage_id, feed_type, feed_id, source_system, source_identifier,
        ingested_at, transformation_chain, quality_checks_passed, metadata_json)
     VALUES (%(lid)s, 'YIELD_CURVE', %(fid)s, %(src)s, %(src)s,
             now(), ARRAY['RECEIVE', 'VALIDATE', 'PARSE', 'STORE'], true,
             %(meta)s::jsonb)
     ON CONFLICT (lineage_id) DO NOTHING;
     ```
     lineage_id = f"curve-{curve_id}-{datetime.utcnow().isoformat()}"
     metadata_json = {"record_count": len(nodes), "vendor": source}
   - Return MarketFeedStatus (feed_id, feed_type, status='PASS', record_count, ingested_at)

2. `GET /api/v1/ingestion/market-feeds/yield-curves/{curve_id}` - Retrieve yield curve
   - Accept optional as_of_date query param
   - If as_of_date provided: SELECT * FROM market_data_feed WHERE feed_id = %(cid)s AND as_of_date = %(aof)s
   - Else: SELECT * FROM market_data_feed WHERE feed_id = %(cid)s ORDER BY as_of_date DESC LIMIT 1
   - Return YieldCurveOut with payload_json

3. `POST /api/v1/ingestion/market-feeds/credit-spreads` - Ingest credit spreads
   - Accept CreditSpreadUpload (issuer_id, spread_type, as_of_date, source, spreads: List[{tenor, spread_bps}])
   - Validate: spread_type IN ('Z_SPREAD', 'OAS', 'ASW_SPREAD')
   - Same UPSERT pattern as yield curves
   - feed_type = 'CREDIT_SPREAD', feed_id = f"{issuer_id}-{spread_type}-{as_of_date}"
   - Record lineage with issuer_id in metadata

4. `GET /api/v1/ingestion/market-feeds/credit-spreads/{issuer_id}` - Retrieve credit spreads
   - Query by issuer_id and optional as_of_date
   - Return CreditSpreadOut

5. `POST /api/v1/ingestion/market-feeds/ratings` - Ingest rating change
   - Accept RatingChange (entity_id, agency, rating, outlook, as_of_date, effective_date, source)
   - Validate: agency IN ('SP', 'MOODYS', 'FITCH', 'DBRS')
   - INSERT into rating_history (not market_data_feed - rating_history has dedicated table)
   - Also record in data_lineage with feed_type='RATING'
   - Return MarketFeedStatus

6. `GET /api/v1/ingestion/market-feeds/ratings/{entity_id}` - Get ratings history
   - Query rating_history table
   - Optional agency filter
   - Return List[RatingHistoryOut]

7. `GET /api/v1/ingestion/market-feeds/status` - List recent feed statuses
   - Accept optional feed_type filter, limit (default 50)
   - SELECT feed_id, feed_type, as_of_date, source, validation_status, created_at
       FROM market_data_feed
       WHERE (%(ft)s IS NULL OR feed_type = %(ft)s)
       ORDER BY created_at DESC LIMIT %(lim)s
   - Return List[MarketFeedStatus]

Use db_conn() for all queries. Handle validation errors by setting validation_status='FAIL' and storing errors in metadata_json. Use psycopg.types.json.Json() for jsonb inserts.
  </action>
  <verify>
```bash
# Start data ingestion service
uvicorn services.data_ingestion_svc.app.main:app --reload --port 8005 &
sleep 3

# Test yield curve ingestion
curl -X POST http://localhost:8005/api/v1/ingestion/market-feeds/yield-curves \
  -H "Content-Type: application/json" \
  -d '{
    "curve_id": "USD-OIS-TEST",
    "curve_type": "DISCOUNT",
    "as_of_date": "2026-02-11",
    "source": "BLOOMBERG",
    "nodes": [
      {"tenor": "1M", "rate": 0.05},
      {"tenor": "3M", "rate": 0.051},
      {"tenor": "6M", "rate": 0.052},
      {"tenor": "1Y", "rate": 0.053}
    ]
  }' | jq .

# Retrieve curve
curl -s http://localhost:8005/api/v1/ingestion/market-feeds/yield-curves/USD-OIS-TEST | jq .

# List feed statuses
curl -s http://localhost:8005/api/v1/ingestion/market-feeds/status | jq .

pkill -f "uvicorn.*data_ingestion"
```
  </verify>
  <done>Market data feed ingestion working. Yield curves, credit spreads, and ratings ingest with UPSERT idempotency. Lineage records created for all ingestions. Status endpoint lists recent feeds with validation results.</done>
</task>

<task type="auto">
  <name>Task 2: Implement loan servicing data ingestion</name>
  <files>services/data_ingestion_svc/app/routes/loan_servicing.py</files>
  <action>
Replace stub implementations in loan_servicing.py with working endpoints for batch position uploads.

**loan_servicing.py endpoints:**

1. `POST /api/v1/ingestion/loan-servicing/batch` - Upload batch of positions
   - Accept LoanServicingBatch (source_file, as_of_date, positions: List[PositionUpload])
   - PositionUpload has: instrument_id, portfolio_node_id, quantity, base_ccy, cost_basis, book_value
   - Generate batch_id = f"batch-{uuid4()}"
   - Validate each position:
     - instrument_id exists in instrument table
     - portfolio_node_id exists in portfolio_node table
     - quantity > 0, cost_basis >= 0, book_value >= 0
     - base_ccy is valid 3-letter currency code
   - Collect validation errors in list
   - INSERT into ingestion_batch:
     ```sql
     INSERT INTO ingestion_batch
       (batch_id, batch_type, source_file, record_count, validation_errors_json,
        status, started_at, completed_at, created_by)
     VALUES (%(bid)s, 'LOAN_SERVICING', %(sf)s, %(rc)s, %(ve)s::jsonb,
             %(st)s, now(), now(), %(cb)s);
     ```
     status = 'COMPLETED' if no validation errors, else 'FAILED'
   - If validation passes:
     - For each position, UPSERT into position table:
       ```sql
       INSERT INTO position
         (position_id, portfolio_node_id, instrument_id, quantity, base_ccy,
          cost_basis, book_value, status, created_at, updated_at)
       VALUES (%(pid)s, %(pnid)s, %(iid)s, %(qty)s, %(ccy)s, %(cb)s, %(bv)s, 'ACTIVE', now(), now())
       ON CONFLICT (portfolio_node_id, instrument_id)
       DO UPDATE SET
         quantity = EXCLUDED.quantity,
         cost_basis = EXCLUDED.cost_basis,
         book_value = EXCLUDED.book_value,
         updated_at = now();
       ```
     - Record lineage for batch with transformation_chain = ['RECEIVE', 'VALIDATE', 'PARSE', 'UPSERT']
   - Return IngestionBatchStatus (batch_id, status, record_count, validation_errors, ingested_at)

2. `GET /api/v1/ingestion/loan-servicing/batch/{batch_id}` - Get batch status
   - SELECT * FROM ingestion_batch WHERE batch_id = %(bid)s
   - Return IngestionBatchStatus

3. `GET /api/v1/ingestion/loan-servicing/batches` - List recent batches
   - Accept optional status filter, limit (default 50)
   - SELECT * FROM ingestion_batch WHERE batch_type = 'LOAN_SERVICING'
       ORDER BY started_at DESC LIMIT %(lim)s
   - Return List[IngestionBatchStatus]

4. `POST /api/v1/ingestion/loan-servicing/reconcile` - Reconcile positions against servicing file
   - Accept ReconcileRequest (portfolio_node_id, as_of_date, expected_positions: List[PositionUpload])
   - Query actual positions: SELECT * FROM position WHERE portfolio_node_id = %(pid)s AND status='ACTIVE'
   - Compare expected vs actual:
     - Missing positions: in expected, not in actual
     - Extra positions: in actual, not in expected
     - Quantity mismatches: different quantity for same instrument
   - Return ReconcileResponse (missing, extra, mismatches, match_count, total_count)

Handle validation errors gracefully. Store all errors in validation_errors_json. Allow partial batch ingestion (ingest valid rows, skip invalid). Log all operations to data_lineage.
  </action>
  <verify>
```bash
# Start data ingestion service
uvicorn services.data_ingestion_svc.app.main:app --reload --port 8005 &
sleep 3

# Setup test data
psql $DATABASE_URL -c "INSERT INTO instrument (instrument_id, instrument_type) VALUES ('loan-1', 'LOAN'), ('loan-2', 'LOAN') ON CONFLICT DO NOTHING;"
psql $DATABASE_URL -c "INSERT INTO portfolio_node (portfolio_node_id, name, node_type) VALUES ('test-book-1', 'Test Book', 'BOOK') ON CONFLICT DO NOTHING;"

# Upload loan servicing batch
curl -X POST http://localhost:8005/api/v1/ingestion/loan-servicing/batch \
  -H "Content-Type: application/json" \
  -d '{
    "source_file": "servicing_20260211.csv",
    "as_of_date": "2026-02-11",
    "positions": [
      {
        "instrument_id": "loan-1",
        "portfolio_node_id": "test-book-1",
        "quantity": 1000000,
        "base_ccy": "USD",
        "cost_basis": 1000000,
        "book_value": 1000000
      },
      {
        "instrument_id": "loan-2",
        "portfolio_node_id": "test-book-1",
        "quantity": 500000,
        "base_ccy": "USD",
        "cost_basis": 500000,
        "book_value": 500000
      }
    ]
  }' | jq .

# List batches
curl -s http://localhost:8005/api/v1/ingestion/loan-servicing/batches | jq .

pkill -f "uvicorn.*data_ingestion"
```
  </verify>
  <done>Loan servicing batch ingestion working. Positions UPSERT with deduplication. Validation errors collected and stored. Reconciliation endpoint compares expected vs actual positions. All batches tracked with audit trail.</done>
</task>

<task type="auto">
  <name>Task 3: Implement data lineage query endpoints</name>
  <files>services/data_ingestion_svc/app/routes/lineage.py</files>
  <action>
Replace stub implementations in lineage.py with working endpoints for lineage tracing and impact analysis.

**lineage.py endpoints:**

1. `GET /api/v1/ingestion/lineage/feed/{feed_id}` - Get lineage for specific feed
   - SELECT * FROM data_lineage WHERE feed_id = %(fid)s ORDER BY ingested_at DESC
   - Return List[LineageOut] with transformation_chain, quality_checks_passed, metadata_json

2. `GET /api/v1/ingestion/lineage/position/{position_id}` - Trace position lineage
   - Query position → instrument → instrument_version → issuer → rating_history
   - Query position → valuation_result → run → marketdata_snapshot
   - Find all data_lineage records contributing to position valuation:
     - Market data feeds used in run's snapshot
     - Rating history entries for position's issuer
     - Servicing batch that created/updated position
   - Return LineageGraphOut with nodes (feeds, batches, ratings) and edges (dependencies)

3. `POST /api/v1/ingestion/lineage/impact-analysis` - Impact analysis for feed
   - Accept ImpactAnalysisRequest (feed_id)
   - Find all runs using this feed:
     - If feed is yield curve: find runs with marketdata_snapshot containing this curve
     - If feed is rating: find positions with issuer having this rating
   - Return ImpactAnalysisResponse (affected_runs: List[run_id], affected_positions: List[position_id])

4. `GET /api/v1/ingestion/lineage/graph` - Get full lineage graph
   - Accept optional filters: feed_type, source_system, date_range
   - Build graph structure:
     - Nodes: data_lineage records (source feeds, transformations, outputs)
     - Edges: dependencies (feed → position, position → valuation)
   - Return LineageGraphOut with nodes and edges in graph format

5. `GET /api/v1/ingestion/lineage/quality-checks` - List quality check failures
   - SELECT * FROM data_lineage WHERE quality_checks_passed = false
       ORDER BY ingested_at DESC LIMIT 100
   - Return List[LineageOut] with failure details in metadata_json

For lineage graph construction, use recursive queries to traverse dependencies. Return graph in format suitable for visualization (nodes with id/type/label, edges with source/target).

All endpoints use db_conn() and handle missing records gracefully (return empty list or 404 as appropriate).
  </action>
  <verify>
```bash
# Start data ingestion service
uvicorn services.data_ingestion_svc.app.main:app --reload --port 8005 &
sleep 3

# Test lineage query (using feed_id from Task 1)
curl -s http://localhost:8005/api/v1/ingestion/lineage/feed/USD-OIS-TEST | jq .

# Test impact analysis
curl -X POST http://localhost:8005/api/v1/ingestion/lineage/impact-analysis \
  -H "Content-Type: application/json" \
  -d '{"feed_id": "USD-OIS-TEST"}' | jq .

# List quality check failures
curl -s http://localhost:8005/api/v1/ingestion/lineage/quality-checks | jq .

pkill -f "uvicorn.*data_ingestion"
```
  </verify>
  <done>Data lineage query endpoints working. Position lineage traces back to all contributing feeds. Impact analysis identifies affected runs and positions. Quality check failures queryable. Graph endpoint returns structure for visualization.</done>
</task>

</tasks>

<verification>
**Market Data Ingestion:**
- [ ] Yield curves ingest with validation and UPSERT idempotency
- [ ] Credit spreads and ratings ingest with lineage tracking
- [ ] Content-addressable hashing prevents duplicate feeds
- [ ] Validation errors stored in metadata_json
- [ ] Feed status endpoint lists recent ingestions

**Loan Servicing:**
- [ ] Batch position upload with validation
- [ ] Foreign key validation on instrument_id and portfolio_node_id
- [ ] UPSERT updates existing positions without duplication
- [ ] Reconciliation compares expected vs actual positions
- [ ] Batch status tracking with audit trail

**Data Lineage:**
- [ ] Feed lineage query returns transformation chain
- [ ] Position lineage traces to all contributing feeds
- [ ] Impact analysis identifies affected runs and positions
- [ ] Quality check failures queryable
- [ ] Graph endpoint returns visualization-ready structure
</verification>

<success_criteria>
**Data ingestion service complete when:**
- User can upload market data (yield curves, spreads, ratings) via API
- User can batch upload loan servicing positions with validation
- All ingested data has lineage records with audit trail
- Data lineage traces position valuation back to source feeds
- Impact analysis identifies which runs/positions use specific feeds
- Validation errors stored and queryable for debugging
- UPSERT idempotency ensures no duplicate data
</success_criteria>

<output>
After completion, create `.planning/phases/03-portfolio-data-services/03-04-SUMMARY.md` documenting:
- Ingestion patterns and validation rules
- UPSERT idempotency strategy
- Data lineage graph structure
- Impact analysis algorithm
</output>
