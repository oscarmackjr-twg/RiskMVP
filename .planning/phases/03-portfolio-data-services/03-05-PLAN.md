---
phase: 03-portfolio-data-services
plan: 05
type: execute
wave: 3
depends_on: ["03-02", "03-03"]
files_modified:
  - services/portfolio_svc/app/routes/snapshots.py
  - services/portfolio_svc/app/models.py
  - services/common/portfolio_queries.py
autonomous: true

must_haves:
  truths:
    - "User can create point-in-time portfolio snapshots with deduplication"
    - "User can query historical snapshots for time-series analysis"
    - "Multi-currency positions aggregate to USD using FX spots"
    - "Snapshot creation handles position aggregation by instrument"
    - "FX spots are queryable by snapshot_id for consistent conversion"
  artifacts:
    - path: "services/portfolio_svc/app/routes/snapshots.py"
      provides: "Portfolio snapshot CRUD and time-series queries"
      exports: ["create_snapshot", "list_snapshots", "compare_snapshots"]
    - path: "services/common/portfolio_queries.py"
      provides: "Multi-currency aggregation with FX conversion"
      exports: ["build_multi_currency_aggregation_query"]
  key_links:
    - from: "services/portfolio_svc/app/routes/snapshots.py"
      to: "portfolio_snapshot table"
      via: "Content-addressable UPSERT with payload_hash"
      pattern: "ON CONFLICT (portfolio_node_id, payload_hash)"
    - from: "multi-currency queries"
      to: "fx_spot table"
      via: "JOIN on snapshot_id for consistent FX rates"
      pattern: "fx.snapshot_id = %(sid)s"
---

<objective>
Implement portfolio snapshot management with time-series tracking and multi-currency aggregation. Enable historical position tracking, snapshot comparison, and consistent FX conversion across all portfolio queries.

Purpose: Completes Phase 3 portfolio domain; enables historical analytics and multi-currency reporting.
Output: Working snapshot service with deduplication, time-series queries, and FX conversion integration.
</objective>

<execution_context>
@C:/Users/omack/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/omack/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-portfolio-data-services/03-RESEARCH.md
@.planning/phases/03-portfolio-data-services/03-01-SUMMARY.md
@.planning/phases/03-portfolio-data-services/03-02-SUMMARY.md
@.planning/phases/03-portfolio-data-services/03-03-SUMMARY.md

# Database schema
@sql/002_portfolio_data_services.sql

# Service code
@services/portfolio_svc/app/routes/snapshots.py
@services/portfolio_svc/app/models.py
@services/common/portfolio_queries.py
@services/common/hash.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement portfolio snapshot creation with deduplication</name>
  <files>
services/portfolio_svc/app/routes/snapshots.py
services/common/portfolio_queries.py
  </files>
  <action>
Replace stub implementations in snapshots.py with working endpoints for snapshot management.

**snapshots.py endpoints:**

1. `POST /api/v1/snapshots` - Create portfolio snapshot
   - Accept SnapshotCreate (portfolio_node_id, as_of_date, include_hierarchy: bool = false)
   - Query current positions for portfolio:
     ```sql
     SELECT
       instrument_id,
       product_type,
       base_ccy,
       SUM(quantity) AS aggregated_quantity,
       COUNT(DISTINCT position_id) AS position_count
     FROM position
     WHERE portfolio_node_id = %(pid)s AND status = 'ACTIVE'
     GROUP BY instrument_id, product_type, base_ccy
     ```
   - If include_hierarchy=true, recursively include all child portfolios using hierarchy CTE
   - Build payload_json:
     ```json
     {
       "portfolio_node_id": "port-123",
       "as_of_date": "2026-02-11T00:00:00Z",
       "include_hierarchy": false,
       "positions": [
         {
           "instrument_id": "loan-1",
           "product_type": "LOAN",
           "base_ccy": "USD",
           "aggregated_quantity": 1000000,
           "position_count": 1
         }
       ],
       "total_positions": 1,
       "total_instruments": 1
     }
     ```
   - Compute payload_hash = sha256_json(payload_json)
   - Check for existing snapshot with same hash:
     ```sql
     SELECT snapshot_id FROM portfolio_snapshot
     WHERE portfolio_node_id = %(pid)s AND payload_hash = %(ph)s
     ```
   - If exists, return existing snapshot_id (idempotent)
   - Else, INSERT new snapshot:
     ```sql
     INSERT INTO portfolio_snapshot
       (snapshot_id, portfolio_node_id, as_of_date, payload_json, payload_hash, created_at)
     VALUES (%(sid)s, %(pid)s, %(aof)s, %(pl)s::jsonb, %(ph)s, now())
     ON CONFLICT (snapshot_id) DO UPDATE SET
       payload_json = EXCLUDED.payload_json,
       payload_hash = EXCLUDED.payload_hash;
     ```
     snapshot_id = f"snap-{portfolio_node_id}-{as_of_date.strftime('%Y%m%d')}-{short_hash}"
   - Return SnapshotOut (snapshot_id, portfolio_node_id, as_of_date, total_positions, created_at)

2. `GET /api/v1/snapshots/{snapshot_id}` - Get snapshot details
   - SELECT * FROM portfolio_snapshot WHERE snapshot_id = %(sid)s
   - Return SnapshotOut with full payload_json

3. `GET /api/v1/snapshots` - List snapshots with filtering
   - Accept query params: portfolio_node_id, date_range (from/to), limit, offset
   - SELECT * FROM portfolio_snapshot
       WHERE (%(pid)s IS NULL OR portfolio_node_id = %(pid)s)
         AND (%(from)s IS NULL OR as_of_date >= %(from)s)
         AND (%(to)s IS NULL OR as_of_date <= %(to)s)
       ORDER BY as_of_date DESC
       LIMIT %(lim)s OFFSET %(off)s
   - Return List[SnapshotOut]

4. `POST /api/v1/snapshots/compare` - Compare two snapshots
   - Accept SnapshotCompareRequest (snapshot_id_1, snapshot_id_2)
   - Load both snapshots
   - Compare positions:
     - New positions: in snapshot_2, not in snapshot_1
     - Removed positions: in snapshot_1, not in snapshot_2
     - Quantity changes: different quantity for same instrument
   - Return SnapshotCompareResponse (new_positions, removed_positions, quantity_changes, summary)

5. `GET /api/v1/snapshots/{portfolio_id}/time-series` - Time-series of snapshots
   - Accept portfolio_node_id path param, date_range query params
   - SELECT snapshot_id, as_of_date, payload_json ->> 'total_positions' AS position_count,
            payload_json ->> 'total_instruments' AS instrument_count
       FROM portfolio_snapshot
       WHERE portfolio_node_id = %(pid)s
         AND as_of_date BETWEEN %(from)s AND %(to)s
       ORDER BY as_of_date ASC
   - Return List[SnapshotTimeSeriesPoint] (as_of_date, position_count, instrument_count)

6. `DELETE /api/v1/snapshots/{snapshot_id}` - Delete snapshot
   - Soft delete or hard delete based on business requirements
   - For audit trail, prefer soft delete: UPDATE status='DELETED'
   - Return 204 No Content

Pattern: All snapshots are immutable once created. Deduplication via payload_hash prevents creating identical snapshots. Time-series queries enable historical analysis without storing redundant data.
  </action>
  <verify>
```bash
# Start portfolio service
uvicorn services.portfolio_svc.app.main:app --reload --port 8004 &
sleep 3

# Create test portfolio and positions (assumes setup from 03-02)
PORT_ID="test-portfolio-1"

# Create snapshot
curl -X POST http://localhost:8004/api/v1/snapshots \
  -H "Content-Type: application/json" \
  -d "{\"portfolio_node_id\": \"$PORT_ID\", \"as_of_date\": \"2026-02-11T00:00:00Z\"}" | jq .

# List snapshots
curl -s "http://localhost:8004/api/v1/snapshots?portfolio_node_id=$PORT_ID" | jq .

# Create second snapshot with changed positions
# (modify position quantity)
psql $DATABASE_URL -c "UPDATE position SET quantity = 2000000 WHERE portfolio_node_id = '$PORT_ID';"

curl -X POST http://localhost:8004/api/v1/snapshots \
  -H "Content-Type: application/json" \
  -d "{\"portfolio_node_id\": \"$PORT_ID\", \"as_of_date\": \"2026-02-12T00:00:00Z\"}" | jq .

# Compare snapshots
SNAP1=$(curl -s "http://localhost:8004/api/v1/snapshots?portfolio_node_id=$PORT_ID&limit=2" | jq -r '.[1].snapshot_id')
SNAP2=$(curl -s "http://localhost:8004/api/v1/snapshots?portfolio_node_id=$PORT_ID&limit=2" | jq -r '.[0].snapshot_id')
curl -X POST http://localhost:8004/api/v1/snapshots/compare \
  -H "Content-Type: application/json" \
  -d "{\"snapshot_id_1\": \"$SNAP1\", \"snapshot_id_2\": \"$SNAP2\"}" | jq .

pkill -f "uvicorn.*portfolio_svc"
```
  </verify>
  <done>Portfolio snapshot creation working. Content-addressable deduplication prevents duplicate snapshots. Snapshot comparison identifies new/removed/changed positions. Time-series queries return historical data. All snapshots immutable.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate FX conversion across all aggregation queries</name>
  <files>services/common/portfolio_queries.py</files>
  <action>
Update all aggregation query builders in portfolio_queries.py to integrate FX conversion using fx_spot table.

**Changes to existing query builders:**

For each aggregation query (issuer, sector, rating, geography, currency, product type), ensure FX conversion pattern:

```sql
CASE
  WHEN pos.base_ccy = 'USD' THEN (vr.measures_json ->> 'PV')::numeric
  ELSE (vr.measures_json ->> 'PV')::numeric * COALESCE(fx.spot_rate, 1.0)
END AS pv_usd
```

Join to fx_spot table:
```sql
LEFT JOIN fx_spot fx ON fx.pair = pos.base_ccy || '/USD'
  AND fx.snapshot_id = %(snapshot_id)s
```

**Add helper function:**

Create get_fx_snapshot_for_run(run_id) -> snapshot_id:
```python
def get_fx_snapshot_for_run(run_id: str) -> str:
    """Get market snapshot_id for run to use for FX conversion."""
    with db_conn() as conn:
        row = conn.execute(
            "SELECT market_snapshot_id FROM run WHERE run_id = %(rid)s",
            {'rid': run_id}
        ).fetchone()
        if not row:
            raise ValueError(f"Run {run_id} not found")
        return row['market_snapshot_id']
```

**Update aggregation endpoints:**

In aggregation.py endpoints (from 03-03), add:
- If run_id provided, get snapshot_id = get_fx_snapshot_for_run(run_id)
- Pass snapshot_id to query builder
- If no run_id, use latest FX spots (snapshot_id=None, query max(as_of_date))

**FX spot management endpoints:**

Add to data_ingestion_svc/app/routes/market_feeds.py:

`POST /api/v1/ingestion/market-feeds/fx-spots` - Ingest FX spots
- Accept FXSpotUpload (snapshot_id, as_of_date, spots: List[{pair, spot_rate}])
- Validate pair format: '{CCY1}/{CCY2}' (e.g., 'EUR/USD', 'GBP/USD')
- UPSERT into fx_spot table:
  ```sql
  INSERT INTO fx_spot (pair, snapshot_id, spot_rate, as_of_date, source, created_at)
  VALUES (%(pair)s, %(sid)s, %(rate)s, %(aof)s, %(src)s, now())
  ON CONFLICT (pair, snapshot_id) DO UPDATE SET
    spot_rate = EXCLUDED.spot_rate,
    as_of_date = EXCLUDED.as_of_date;
  ```
- Return FXSpotStatus

`GET /api/v1/ingestion/market-feeds/fx-spots/{snapshot_id}` - Get FX spots for snapshot
- SELECT * FROM fx_spot WHERE snapshot_id = %(sid)s
- Return List[FXSpotOut]

Ensure all portfolio metrics and aggregation endpoints now use consistent FX conversion tied to run's snapshot_id.
  </action>
  <verify>
```bash
# Start services
uvicorn services.portfolio_svc.app.main:app --reload --port 8004 &
uvicorn services.data_ingestion_svc.app.main:app --reload --port 8005 &
sleep 3

# Ingest FX spots (tied to market snapshot)
SNAPSHOT_ID="test-snapshot-1"
curl -X POST http://localhost:8005/api/v1/ingestion/market-feeds/fx-spots \
  -H "Content-Type: application/json" \
  -d "{
    \"snapshot_id\": \"$SNAPSHOT_ID\",
    \"as_of_date\": \"2026-02-11\",
    \"spots\": [
      {\"pair\": \"EUR/USD\", \"spot_rate\": 1.08},
      {\"pair\": \"GBP/USD\", \"spot_rate\": 1.27},
      {\"pair\": \"JPY/USD\", \"spot_rate\": 0.0067}
    ]
  }" | jq .

# Create multi-currency positions
# (add EUR and GBP positions to test portfolio)

# Query aggregation with FX conversion
PORT_ID="test-portfolio-1"
RUN_ID="test-run-1"
curl -s "http://localhost:8004/api/v1/aggregation/$PORT_ID/by-currency?run_id=$RUN_ID" | jq .

# Verify totals match across dimensions
curl -s "http://localhost:8004/api/v1/aggregation/$PORT_ID/metrics?run_id=$RUN_ID" | jq '.metrics.market_value_usd'

pkill -f "uvicorn.*(portfolio_svc|data_ingestion)"
```
  </verify>
  <done>FX conversion integrated across all aggregation queries. FX spots tied to market snapshot_id for consistency. Multi-currency positions aggregate correctly to USD. All portfolio metrics use same FX rates from run's snapshot.</done>
</task>

<task type="auto">
  <name>Task 3: Implement concentration risk monitoring (RISK-06 from Phase 2)</name>
  <files>services/portfolio_svc/app/routes/aggregation.py</files>
  <action>
Add concentration monitoring endpoints to complete RISK-06 requirement (deferred from Phase 2).

**New endpoints in aggregation.py:**

1. `GET /api/v1/aggregation/{portfolio_id}/concentration` - Portfolio concentration analysis
   - Accept portfolio_node_id, optional run_id, concentration_threshold (default 10%)
   - Compute concentration by multiple dimensions:
     - **Issuer concentration**: Top N issuers representing >X% of portfolio
     - **Sector concentration**: Sectors >X% of portfolio
     - **Geographic concentration**: Countries >X% of portfolio
     - **Single name concentration**: Any single position >X% of portfolio
   - Return ConcentrationReport with:
     - violations: List[{dimension, entity, weight_pct, threshold_pct}]
     - top_10_issuers: List[{issuer, weight_pct}]
     - top_5_sectors: List[{sector, weight_pct}]
     - herfindahl_index: Sum of squared weights (concentration measure)
     - diversification_ratio: 1 / sqrt(herfindahl_index)

2. `GET /api/v1/aggregation/{portfolio_id}/rating-migration` - Rating migration tracking
   - Accept portfolio_node_id, date_range (from/to)
   - Query rating_history for all issuers in portfolio
   - Identify rating changes within date range:
     ```sql
     WITH position_issuers AS (
       SELECT DISTINCT (iv.terms_json ->> 'issuer_id') AS issuer_id
       FROM position pos
       JOIN instrument_version iv ON pos.instrument_id = iv.instrument_id
       WHERE pos.portfolio_node_id = %(pid)s AND pos.status = 'ACTIVE'
     )
     SELECT rh1.entity_id, rh1.rating AS old_rating, rh2.rating AS new_rating,
            rh1.as_of_date AS old_date, rh2.as_of_date AS new_date
     FROM rating_history rh1
     JOIN rating_history rh2 ON rh1.entity_id = rh2.entity_id
       AND rh1.agency = rh2.agency
       AND rh2.as_of_date > rh1.as_of_date
     WHERE rh1.entity_id IN (SELECT issuer_id FROM position_issuers)
       AND rh1.as_of_date >= %(from)s AND rh2.as_of_date <= %(to)s
       AND rh1.rating != rh2.rating
     ORDER BY rh2.as_of_date DESC;
     ```
   - For each migration, compute:
     - Direction: upgrade, downgrade, or watchlist change
     - Notches: Number of rating steps changed
     - Portfolio exposure: Sum of PV for positions with this issuer
   - Return RatingMigrationReport with migrations and portfolio impact

3. `POST /api/v1/aggregation/{portfolio_id}/concentration/alert` - Set concentration alert thresholds
   - Accept ConcentrationAlertConfig (dimension, threshold_pct, alert_email)
   - Store in portfolio_node.metadata_json under 'concentration_alerts' key
   - Return stored config

**Concentration calculations:**

- **Herfindahl Index**: H = Σ(wi²) where wi is weight of position i
  - H = 1.0 means all portfolio in one position (max concentration)
  - H → 0 means perfect diversification
- **Single name limit**: Any issuer > 10% of portfolio
- **Sector limit**: Any sector > 25% of portfolio
- **Geographic limit**: Any country > 40% of portfolio

Use same aggregation query patterns from earlier tasks. Add concentration logic as post-processing in Python or via SQL window functions.
  </action>
  <verify>
```bash
# Start portfolio service
uvicorn services.portfolio_svc.app.main:app --reload --port 8004 &
sleep 3

# Test concentration analysis
PORT_ID="test-portfolio-1"
curl -s "http://localhost:8004/api/v1/aggregation/$PORT_ID/concentration?concentration_threshold=5" | jq .

# Check for violations
curl -s "http://localhost:8004/api/v1/aggregation/$PORT_ID/concentration" | jq '.violations'

# Test rating migration tracking
curl -s "http://localhost:8004/api/v1/aggregation/$PORT_ID/rating-migration?from=2026-01-01&to=2026-02-11" | jq .

pkill -f "uvicorn.*portfolio_svc"
```
  </verify>
  <done>Concentration monitoring working with multi-dimensional analysis. Rating migration tracking identifies upgrades/downgrades with portfolio impact. Alert thresholds configurable per portfolio. Herfindahl index and diversification ratio calculated. RISK-06 requirement complete.</done>
</task>

</tasks>

<verification>
**Portfolio Snapshots:**
- [ ] Create snapshots with position aggregation and deduplication
- [ ] Content-addressable hashing prevents duplicate snapshots
- [ ] Snapshot comparison identifies new/removed/changed positions
- [ ] Time-series queries return historical position counts
- [ ] Snapshots are immutable once created

**Multi-Currency Aggregation:**
- [ ] FX spots stored per market snapshot_id
- [ ] All aggregation queries use consistent FX conversion
- [ ] Portfolio metrics convert to USD correctly
- [ ] Currency breakdown shows distribution by currency
- [ ] Missing FX spots default to 1.0 with warning

**Concentration Monitoring:**
- [ ] Issuer, sector, geography concentration calculated
- [ ] Violations identified when exceeding thresholds
- [ ] Herfindahl index and diversification ratio computed
- [ ] Rating migration tracking with portfolio impact
- [ ] Alert thresholds configurable per portfolio
</verification>

<success_criteria>
**Snapshots and multi-currency complete when:**
- User can create point-in-time snapshots with deduplication
- Historical snapshots queryable for time-series analysis
- Snapshot comparison identifies position changes
- Multi-currency positions aggregate consistently to USD using snapshot-scoped FX rates
- FX conversion integrated across all portfolio queries
- Concentration monitoring identifies violations by dimension
- Rating migration tracking shows upgrades/downgrades with impact
- All Phase 3 requirements (PORT-01 through PORT-08, DATA-01 through DATA-04, RISK-06) delivered
</success_criteria>

<output>
After completion, create `.planning/phases/03-portfolio-data-services/03-05-SUMMARY.md` documenting:
- Snapshot deduplication strategy and content-addressable hashing
- Multi-currency aggregation patterns with FX conversion
- Concentration monitoring algorithms and thresholds
- Rating migration tracking methodology
- Phase 3 completion summary with all requirements verified
</output>
