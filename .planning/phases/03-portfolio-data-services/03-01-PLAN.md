---
phase: 03-portfolio-data-services
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - sql/002_portfolio_data_services.sql
  - services/common/db.py
autonomous: true

must_haves:
  truths:
    - "Portfolio nodes can be stored with parent-child relationships"
    - "Positions link to portfolio nodes and instruments"
    - "Reference data tables support issuer, sector, rating, geography lookups"
    - "Market data feeds and lineage records can be stored"
    - "Multi-currency positions and FX spots are queryable"
  artifacts:
    - path: "sql/002_portfolio_data_services.sql"
      provides: "Database schema for Phase 3 domain"
      min_lines: 200
      contains: "CREATE TABLE portfolio_node"
    - path: "services/common/db.py"
      provides: "Connection pooling verification"
      exports: ["db_conn"]
  key_links:
    - from: "sql/002_portfolio_data_services.sql"
      to: "sql/001_mvp_core.sql"
      via: "Foreign key references"
      pattern: "REFERENCES (instrument|run|valuation_result)"
---

<objective>
Extend database schema with portfolio hierarchy, reference data, and data ingestion tables. Enable all Phase 3 services to store and query portfolio, position, reference, and lineage data.

Purpose: Foundation for portfolio and data services; all subsequent plans depend on this schema.
Output: Complete Phase 3 database schema with indexes and constraints; verified connection pooling.
</objective>

<execution_context>
@C:/Users/omack/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/omack/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-portfolio-data-services/03-RESEARCH.md

# Existing schema
@sql/001_mvp_core.sql

# Shared database utilities
@services/common/db.py
@services/common/service_base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Phase 3 database schema extension</name>
  <files>sql/002_portfolio_data_services.sql</files>
  <action>
Create comprehensive SQL migration adding tables for portfolio hierarchy, reference data, and data ingestion:

**Portfolio Domain Tables:**
- `portfolio_node` (portfolio_node_id PK, parent_id FK self-referencing, name, node_type, tags_json, metadata_json, created_at)
  - Index: (parent_id) for recursive hierarchy queries
  - Check: node_type IN ('FUND', 'DESK', 'BOOK', 'STRATEGY')

- `position` (position_id PK, portfolio_node_id FK, instrument_id FK, quantity, base_ccy, cost_basis, book_value, tags_json, status, created_at, updated_at)
  - Index: (portfolio_node_id, instrument_id) for position lookups
  - Index: (instrument_id) for reverse lookups
  - Check: status IN ('ACTIVE', 'CLOSED', 'DELETED')
  - FK: instrument_id REFERENCES instrument(instrument_id)

- `portfolio_snapshot` (snapshot_id PK, portfolio_node_id FK, as_of_date, payload_json, payload_hash, created_at)
  - Index: (portfolio_node_id, as_of_date DESC) for time-series queries
  - Unique: (portfolio_node_id, payload_hash) for deduplication

**Reference Data Tables:**
- `reference_data` (entity_id PK, entity_type, name, ticker, cusip, isin, sector, geography, currency, parent_entity_id, metadata_json, created_at, updated_at)
  - Check: entity_type IN ('ISSUER', 'SECTOR', 'GEOGRAPHY', 'CURRENCY')
  - Index: (entity_type, name) for lookups
  - Index: (ticker), (cusip), (isin) for identifier searches

- `rating_history` (rating_id PK, entity_id FK, agency, rating, outlook, as_of_date, effective_date, metadata_json, created_at)
  - FK: entity_id REFERENCES reference_data(entity_id)
  - Index: (entity_id, as_of_date DESC) for temporal queries
  - Check: agency IN ('SP', 'MOODYS', 'FITCH', 'DBRS')

- `fx_spot` (pair PK, snapshot_id, spot_rate, as_of_date, source, created_at)
  - Composite PK: (pair, snapshot_id)
  - Index: (snapshot_id) for snapshot-scoped FX lookups
  - Check: pair format '{CCY1}/{CCY2}' (e.g., 'EUR/USD')

**Data Ingestion Tables:**
- `market_data_feed` (feed_id PK, feed_type, as_of_date, source, payload_json, payload_hash, validation_status, created_at, updated_at)
  - Check: feed_type IN ('YIELD_CURVE', 'CREDIT_SPREAD', 'FX_SPOT', 'RATING')
  - Check: validation_status IN ('PENDING', 'PASS', 'WARN', 'FAIL')
  - Index: (feed_type, as_of_date DESC)

- `data_lineage` (lineage_id PK, feed_type, feed_id, source_system, source_identifier, ingested_at, transformation_chain, quality_checks_passed, metadata_json, created_at)
  - Index: (feed_id) for lineage tracing
  - transformation_chain stored as text[] (ordered list of steps)

- `ingestion_batch` (batch_id PK, batch_type, source_file, record_count, validation_errors_json, status, started_at, completed_at, created_by)
  - Check: batch_type IN ('LOAN_SERVICING', 'MARKET_DATA', 'POSITION_UPLOAD')
  - Check: status IN ('STARTED', 'VALIDATING', 'COMPLETED', 'FAILED')

**All tables:**
- Use text for IDs (UUIDs or semantic IDs)
- Use timestamptz for all timestamps
- Use jsonb for flexible metadata
- Include created_at, updated_at where appropriate
- Add CHECK constraints for enums
- Add indexes for foreign keys and temporal queries
- Use ON DELETE CASCADE for dependent records where appropriate

Follow 001_mvp_core.sql patterns: BEGIN/COMMIT transaction, IF NOT EXISTS guards, consistent naming conventions (snake_case).
  </action>
  <verify>
```bash
# Syntax validation
psql -h localhost -U postgres -d iprs -f sql/002_portfolio_data_services.sql --dry-run

# Count tables created
grep -c "CREATE TABLE" sql/002_portfolio_data_services.sql
# Expected: 9 tables

# Verify indexes
grep -c "CREATE INDEX" sql/002_portfolio_data_services.sql
# Expected: >=12 indexes
```
  </verify>
  <done>SQL migration file exists with 9 tables, all required indexes, CHECK constraints on enums, and foreign key references to Phase 1 tables. Migration is idempotent and can be applied to existing database.</done>
</task>

<task type="auto">
  <name>Task 2: Verify database connection pooling</name>
  <files>services/common/db.py</files>
  <action>
Review existing db.py to confirm connection pooling is adequate for Phase 3 query load.

Phase 3 services will have higher query volume than Phase 2 (portfolio aggregations, multi-table joins). Verify:
- Connection pooling enabled (if using connection pool)
- Context manager pattern working (with db_conn() as conn)
- Transactions roll back on exceptions
- No connection leaks

If db.py uses simple connection without pooling, document in code comment that pooling can be added if load testing shows need. For MVP with <10K positions and <100 concurrent requests, single connection per request is sufficient.

If connection pooling is missing and would benefit Phase 3, add psycopg3.pool.ConnectionPool:
```python
from psycopg_pool import ConnectionPool
pool = ConnectionPool(os.environ['DATABASE_URL'], min_size=2, max_size=10)
```

Otherwise, keep existing pattern and add comment noting future enhancement.
  </action>
  <verify>
```bash
# Check if connection pooling present
grep -i "pool" services/common/db.py

# Verify context manager exists
grep "def db_conn" services/common/db.py
```
  </verify>
  <done>Connection pattern verified; either pooling is present or documented as future enhancement. db_conn() context manager confirmed working for all Phase 3 services.</done>
</task>

<task type="auto">
  <name>Task 3: Apply schema and validate structure</name>
  <files>sql/002_portfolio_data_services.sql</files>
  <action>
Apply Phase 3 schema extension to local PostgreSQL database and validate table structure.

```bash
# Set DATABASE_URL if not already set
export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/iprs"

# Apply migration
psql $DATABASE_URL -f sql/002_portfolio_data_services.sql

# Validate tables exist
psql $DATABASE_URL -c "\dt" | grep -E "(portfolio_node|position|reference_data|rating_history|fx_spot|market_data_feed|data_lineage|ingestion_batch|portfolio_snapshot)"

# Check foreign keys
psql $DATABASE_URL -c "SELECT conname, conrelid::regclass, confrelid::regclass FROM pg_constraint WHERE contype = 'f' AND conrelid::regclass::text LIKE '%portfolio%' OR conrelid::regclass::text LIKE '%reference%';"

# Verify indexes
psql $DATABASE_URL -c "\di" | grep -E "(portfolio|position|reference|rating|fx_spot|lineage)"
```

If any errors occur during migration (duplicate tables, constraint violations), fix SQL and re-run. Use DROP TABLE IF EXISTS for idempotency if needed (but preserve existing data from 001_mvp_core.sql).

After successful application, insert a test row into each table to verify constraints and defaults work:
```sql
INSERT INTO portfolio_node (portfolio_node_id, name, node_type) VALUES ('test-port-1', 'Test Portfolio', 'FUND');
INSERT INTO reference_data (entity_id, entity_type, name) VALUES ('test-issuer-1', 'ISSUER', 'Test Issuer');
DELETE FROM portfolio_node WHERE portfolio_node_id = 'test-port-1';
DELETE FROM reference_data WHERE entity_id = 'test-issuer-1';
```
  </action>
  <verify>
```bash
# Verify all 9 tables exist
psql $DATABASE_URL -c "SELECT count(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name IN ('portfolio_node', 'position', 'portfolio_snapshot', 'reference_data', 'rating_history', 'fx_spot', 'market_data_feed', 'data_lineage', 'ingestion_batch');"
# Expected: 9

# Check test insert/delete succeeded
psql $DATABASE_URL -c "SELECT 'schema validated' AS status;"
```
  </verify>
  <done>Phase 3 database schema applied successfully. All 9 tables exist with correct structure, indexes, foreign keys, and constraints. Test inserts/deletes confirm validation rules work.</done>
</task>

</tasks>

<verification>
**Schema Structure:**
- [ ] All 9 Phase 3 tables created: portfolio_node, position, portfolio_snapshot, reference_data, rating_history, fx_spot, market_data_feed, data_lineage, ingestion_batch
- [ ] Foreign keys reference existing tables (instrument, run, valuation_result) correctly
- [ ] All indexes created for hierarchy queries, temporal lookups, foreign key joins
- [ ] CHECK constraints enforce valid enum values

**Database Connectivity:**
- [ ] Connection pattern verified for Phase 3 query load
- [ ] Context manager (db_conn) working for transaction management
- [ ] Migration is idempotent and can be re-applied safely

**Validation:**
- [ ] Schema applied to local PostgreSQL without errors
- [ ] Test inserts confirm constraints work
- [ ] Tables queryable from psql and Python
</verification>

<success_criteria>
**Phase 3 database foundation complete when:**
- All 9 tables exist in database with correct structure
- Migration file is idempotent and version-controlled
- Foreign keys link to Phase 1/2 tables correctly
- Indexes support recursive hierarchy queries and temporal lookups
- Connection pooling strategy documented or implemented
- Test rows can be inserted and queried without errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-portfolio-data-services/03-01-SUMMARY.md` documenting:
- Tables created with row counts
- Index strategy for performance
- Connection pooling decision
- Any deviations from research recommendations
</output>
